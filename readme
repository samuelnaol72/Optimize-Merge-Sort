	(1). Explain how your standard (Part A) msort() works briefly. 
	
- The standard msort() is essentially an implementation of the pseudocode provided by the professor during the class lecture. The core algorithm follows the classic Merge Sort technique, where the 
- array is recursively divided into smaller segments and then merged in sorted order(i.e Divide and conquer approach).
- However, some important edge cases were handled to ensure robustness: During the merging phase, if rightPtr (which points to elements in the right half) reaches beyond the bounds of the base array 
- while leftPtr still has elements remaining, the loop terminates, and the remaining elements from the left half are directly copied into the temporary output array. Similarly, if leftPtr completes 
- while rightPtr still has elements left, the remaining right half elements are directly copied to the output array. Finally, the result stored in the temporary output array is copied back to the base 
- array.
	(2). Explain your optimizations for part B.

- As the professor mentioned in the campuswire, the dataset is expected to be of the following type:
	- (a) Half of the dataset is almost sorted(i.e. worst-case of when the list is fully sorted and the next-to-worse case of when it has a single inversion).
	- (b) A very high propportion of the dataset is the short array.
		
- So, in order to tailor to the above dataset, the following approaches were followed in the optimization:
	- Case (a): To handle the almost sorted case efficiently, I implemented a function called check_sorted_status() that determines whether the array is fully sorted, has a single inversion, or is 
	- completely unsorted. This function operates in linear time i.e O(n) ).For example, when check_sorted_status() detects a single inversion, it swaps the elements and checks the array once more. 		- If the array becomes sorted, we avoid further sorting operations. if not, we proceed to the next step.
	
	- Case (b): For smaller arrays (determined by the threshold of 29, as detailed in (3) below), the insertion_sort function is used. This is because insertion sort performs better on small 
	- datasets compared to un-optimized merge sort due to its low overhead and efficient handling of short lists.
	
- An additional optimization was applied during the merge phase of the unoptimized merge sort. For example, if leftPtr completes before rightPtr, the remaining elements from the right half are not
- copied into the temporary array, as they are already in the correct position in the original array. This reduces unnecessary data movement and improves efficiency.
	
	(3). (Optional) provide evidence for your selected threshold (between long and shortlists) leading to optimal performance (extra credit of 1%). 
	
- The threshold selection was based on multiple trials comparing the performance of insertion sort and the unoptimized merge sort across various array lengths. From the results, insertion sort
- consistently outperformed merge sort for arrays of length less than 29.
			Array Length  Average Unoptimized Merge Sort Time(sec)      Average Insertion Sort Time(sec)
				0	        0.000000086					0.000000079
				1               0.000000093					0.000000082
				2               0.000000173					0.000000117
				3               0.000000297					0.000000178
				4               0.000000396					0.000000260
				5               0.000000581					0.000000436
				.		 .						.
				.		 .						.
				.		 .						.
				
				27 		0.000002379					0.000002654
				28 		0.000002012					0.000002001
				29 		0.000002468					0.000002927
				30		0.000002221					0.000002300
				31		0.000002714					0.000003327
				32		0.000002351					0.000002527
				33		0.000002967					0.000003732
				34		0.000002623					0.000002841
				35		0.000003185					0.000004101
				36		0.000002894					0.000003210
				37		0.000003442					0.000004539


	(4). Performance test	
	
- In order to test the optimized merge sort, I have created a random 100 batches of syntetic dataset imitating the expected dataset. Each batch contains 1000 arrays of which 80% is of size between 
- (2~64) and 500 of them satisfy almost sorted condition. As we can see from the following sample tests, optimized merged sort is performing better than the un-optimized merge sort.
	- x@Bee:~/Desktop/2024_Fall/Data Structure and Algorithm(EE205)/Assignments/Projects/Project 1-MergeSort_src/src$ ./test_opt 
		Overall Results: 
		Total Optimized Merge Sort Time: 0.108412373 seconds
		Total Unoptimized Merge Sort Time: 0.123453858 seconds
		Total Insertion Sort Time: 14.184427835 seconds
		Results written to batch_results.csv
	- x@Bee:~/Desktop/2024_Fall/Data Structure and Algorithm(EE205)/Assignments/Projects/Project 1-MergeSort_src/src$ ./test_opt 
		Overall Results: 
		Total Optimized Merge Sort Time: 0.105663643 seconds
		Total Unoptimized Merge Sort Time: 0.124197349 seconds
		Total Insertion Sort Time: 12.412969796 seconds
		Results written to batch_results.csv
	- x@Bee:~/Desktop/2024_Fall/Data Structure and Algorithm(EE205)/Assignments/Projects/Project 1-MergeSort_src/src$ ./test_opt 
		Overall Results: 
		Total Optimized Merge Sort Time: 0.118088649 seconds
		Total Unoptimized Merge Sort Time: 0.134245766 seconds
		Total Insertion Sort Time: 15.816902113 seconds
		Results written to batch_results.csv
		
	
